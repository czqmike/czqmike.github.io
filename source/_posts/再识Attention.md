---
title: å†è¯†Attention Mechanism
date: 2021-05-06 18:12:56
tags: 
- Deep Learning
- Attention Mechanism
- Transformer
- Bert
---

> æ¥å†æ¬¡åˆ·ä¸‹Attentionæœºåˆ¶ã€‚è¿™æ¬¡å‡†å¤‡ä»Attentionçš„åŸç†ï¼Œåˆ°Transformerçš„æ„æˆã€åŸç†ã€åº”ç”¨ã€ä¼˜ç¼ºç‚¹ï¼Œå†åˆ°Bertçš„åŸºæœ¬åŸç†ï¼ŒBertçš„å˜ç§å¥½å¥½æ¢³ç†ä¸€éã€‚

## Attention Mechanism
Attentionæœ¬è´¨ä¸Šå°±æ˜¯ä¸€ä¸ªå†…å®¹è¡¨ç¤º**å‘é‡** (vector)ï¼Œè¡¨ç¤ºä¹‹å‰çš„å„ä¸ªå…ƒç´ å¯¹ä¸‹ä¸ªå…ƒç´ äº§ç”Ÿçš„å½±å“ï¼Œè¿™ç§æœºåˆ¶å¯ä»¥å»ºç«‹å…¶åºåˆ—ä¹‹é—´çš„é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œå¹¶ä¸”å¾—ç›Šäºå¹¶è¡ŒåŒ–ï¼ŒAttentionçš„é€Ÿåº¦è¦å¿«äºLSTMç­‰ä¼ ç»Ÿçš„å»ºç«‹ä¾èµ–å…³ç³»çš„ç»“æ„ã€‚
è¿™ä¸ªå‘é‡çš„è¯¦ç»†äº§ç”Ÿæœºåˆ¶å¯ä»¥å‚è§ç¬”è€…çš„è¿™ç¯‡[åˆæ¢Attention Mechanism](http://czqmike-server.cn/2020/11/13/%E5%88%9D%E6%8E%A2Attention-Mechanism/)ã€‚
ä»Šå¤©æ¥è¯¦ç»†å­¦ä¹ ä¸‹Attentionçš„åˆ†æ”¯ï¼šSoft Attentionã€Hard Attentionã€Local Attentionã€Global Attentionã€Self Attention and Attention over Attention...ï¼ˆæ™•äº†å·²ç»ğŸ˜¢ï¼‰

è¿™äº›Attentionçš„æ–¹å¼æŒ‰ç…§è®¡ç®—åŒºåŸŸã€æ‰€ç”¨ä¿¡æ¯ã€ç»“æ„å±‚æ¬¡ç­‰æ–¹é¢æ¥åˆ†ç±»ã€‚

![Attentionåˆ†ç±»](Attention_Classifications.png)

### è®¡ç®—åŒºåŸŸ
æŒ‰ç…§è®¡ç®—åŒºåŸŸåˆ’åˆ†çš„è¯ï¼Œå¯ä»¥åˆ†ä¸º
1. Soft Attention    
  æ¯”è¾ƒå¸¸è§çš„Attentionæ–¹å¼ï¼Œå¯¹æ‰€æœ‰keyæ±‚æƒé‡æ¦‚ç‡ï¼Œæ¯ä¸ªkeyéƒ½æœ‰ä¸€ä¸ªå¯¹åº”çš„æƒé‡ï¼Œæ˜¯ä¸€ç§**å…¨å±€**çš„è®¡ç®—æ–¹å¼ï¼Œï¼ˆä¹Ÿå¯ä»¥å«Global Attentionï¼‰ã€‚è¿™ç§æ–¹å¼æ¯”è¾ƒç†æ€§ï¼Œå‚è€ƒäº†æ‰€æœ‰keyçš„å†…å®¹å†è¿›è¡ŒåŠ æƒï¼Œä½†æ˜¯è®¡ç®—é‡å¯èƒ½ä¼šæ¯”è¾ƒå¤§ã€‚
2. Hard Attention
  è¿™ç§æ–¹å¼åˆ™æ˜¯**ç²¾å‡†å®šä½**åˆ°æŸä¸ªkeyï¼Œå…¶ä½™keyå°±éƒ½ä¸ç®¡äº†ã€‚ç›¸å½“äºè¿™ä¸ªkeyçš„æ¦‚ç‡æ˜¯1ï¼Œå…¶ä½™keyçš„æ¦‚ç‡å…¨éƒ¨æ˜¯0ã€‚è¿™ç§å¯¹é½æ–¹å¼è¦æ±‚å¾ˆé«˜ï¼Œè¦æ±‚ä¸€æ­¥åˆ°ä½ï¼Œå¦‚æœæ²¡æœ‰æ­£ç¡®å¯¹é½ï¼Œä¼šå¸¦æ¥å¾ˆå¤§çš„å½±å“ã€‚
  å¦ä¸€æ–¹é¢ï¼Œå› ä¸ºå…¶ä¸å¯å¯¼ï¼Œæ‰€ä»¥ä¸€èˆ¬éœ€è¦ç”¨å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•è¿›è¡Œè®­ç»ƒã€‚
3. Local Attention
  è¿™ç§æ–¹å¼æ˜¯ä¸Šè¿°ä¸¤ç§æ–¹å¼çš„ä¸€ä¸ªæŠ˜ä¸­ã€‚å…ˆç”¨Hard Attentionå®šä½åˆ°æŸä¸ªåœ°æ–¹ï¼Œå†ä»¥è¿™ä¸ªåœ°æ–¹ä¸ºä¸­å¿ƒå¾—åˆ°ä¸€ä¸ªçª—å£åŒºåŸŸï¼Œåœ¨è¿™ä¸ªçª—å£åŒºåŸŸä¸­é‡‡ç”¨Soft Attentionã€‚

### æ‰€ç”¨ä¿¡æ¯
ç°åœ¨å‡è®¾æˆ‘ä»¬è¦å¯¹ä¸€ä¸ªå¥å­$S_{i}$è®¡ç®—Attentionï¼Œä½†æ˜¯ä¸€æ®µä¸­è¿˜æœ‰å…¶ä»–çš„å¾ˆå¤šå¥å­$S_{m}, S_{n}$å•Šã€‚é‚£ä¹ˆç”¨ä¸ç”¨å…¶ä»–å¥å­çš„ä¿¡æ¯æ¥Attentionå°±æ˜¯ä¸€ä¸ªå¾ˆå¤§çš„åŒºåˆ«ã€‚
æˆ‘ä»¬æŠŠè¿™ç§å…¶ä»–å¥å­$S_{m}, S_{n}...$ç§°ä½œ**å¤–éƒ¨ä¿¡æ¯**ã€‚
å¦‚æœAttentionä½¿ç”¨äº†å¤–éƒ¨ä¿¡æ¯ï¼Œåˆ™ç§°å…¶ä¸º`General Attention`ï¼Œå¦åˆ™åˆ™ä¸º`Local Attention`ã€‚

### ç»“æ„å±‚æ¬¡
æ ¹æ®ç»“æ„æ¥åˆ’åˆ†çš„è¯ï¼Œå¯ä»¥åˆ†ä¸ºï¼š
  1. å•å±‚Attention
    æ¯”è¾ƒæ™®éçš„åšæ³•ï¼Œå³ä¸€ä¸ªqueryåªå¯¹ä¸€æ®µæ–‡æœ¬è¿›è¡Œä¸€æ¬¡attention
  2. å¤šå±‚Attention
    ä¸€èˆ¬ç”¨äºæ–‡æœ¬å…·æœ‰å±‚æ¬¡å…³ç³»çš„æ¨¡å‹ã€‚
    å‡è®¾æˆ‘ä»¬æŠŠä¸€ä¸ªdocumentåˆ’åˆ†æˆå¤šä¸ªå¥å­ï¼Œåœ¨ç¬¬ä¸€å±‚ï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ªå¥å­ä½¿ç”¨attentionè®¡ç®—å‡ºä¸€ä¸ªå¥å‘é‡ï¼Œè¡¨ç¤ºå¯¹å¥å­ç‰¹å¾çš„ç†è§£ï¼›
    åœ¨ç¬¬äºŒå±‚ï¼Œæˆ‘ä»¬å†å¯¹æ‰€æœ‰å¥å‘é‡åšä¸€æ¬¡Attentionï¼Œè®¡ç®—å‡ºä¸€ä¸ªæ–‡æ¡£å‘é‡ï¼Œè¡¨ç¤ºå¯¹å¥å­é—´å…³ç³»çš„ç†è§£ã€‚æœ€åç”¨è¿™ä¸ªæ–‡æ¡£å‘é‡å»åšä»»åŠ¡ã€‚
    å€¼å¾—ä¸€æçš„æ˜¯ï¼Œå…¶å®ç¬¬ä¸€å±‚å’Œç¬¬äºŒå±‚éƒ½æ˜¯ä¸€ä¸ªå•å±‚Attentionï¼Œåªä¸è¿‡ç»„åˆèµ·æ¥åŠ æ·±æ¨¡å‹çš„ç†è§£è€Œå·²ã€‚
  3. å¤šå¤´Attention (Multi-head Attention)
    ç”¨å¤šä¸ªqueryå¯¹ä¸€æ®µæ–‡æœ¬åšäº†å¤šæ¬¡attentionï¼Œæ¯ä¸ªqueryå…³æ³¨åŸæ–‡çš„ä¸åŒéƒ¨åˆ†ï¼ˆå¦‚q1å…³æ³¨å¥æ³•ï¼Œq2å…³æ³¨å®ä½“é—´å…³ç³»...ï¼‰ã€‚å…¶å®å°±ç›¸å½“äºé‡å¤åšå¤šæ¬¡å•å±‚attention

## Transformer
Transformer, å³åªé‡‡ç”¨Attentionè€Œä¸ç»“åˆCNNæˆ–æ˜¯RNNçš„æ¶æ„ï¼Œåœ¨å„é¡¹ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†ä¸é”™çš„æˆç»©ã€‚
Transformerçš„ä½œè€…è®¤ä¸ºï¼ŒRNN / CNNçš„è®¡ç®—é¡ºåºæ˜¯ä»å·¦åˆ°å³å›ºå®šçš„ï¼Œè€Œè¿™æ ·å›ºå®šçš„é¡ºåºä¼šå¸¦æ¥ä¸¤ä¸ªåå¤„ï¼š
1. æ—¶é—´ç‰‡$t$çš„è®¡ç®—ä¾èµ–äº$t - 1$æ—¶åˆ»çš„è®¡ç®—ç»“æœï¼Œé™åˆ¶äº†æ¨¡å‹çš„å¹¶è¡Œèƒ½åŠ›ã€‚
2. ç”±äºé¡ºåºè®¡ç®—çš„è¿‡ç¨‹ä¸­ä¿¡æ¯ä¼šä¸¢å¤±ï¼Œæ‰€ä»¥å¯¹äºç‰¹åˆ«é•¿è·ç¦»çš„ä¾èµ–ï¼ŒLSTMä»ç„¶æ— èƒ½ä¸ºåŠ›ã€‚
Transformerçš„æå‡ºè§£å†³äº†ä¸Šè¿°ä¸¤ä¸ªé—®é¢˜ï¼šé¦–å…ˆå®ƒä¸éœ€è¦ç­‰å¾…å‰ä¸€æ—¶åˆ»çš„çŠ¶æ€ï¼Œå…·æœ‰è‰¯å¥½çš„å¹¶è¡Œæ€§ï¼Œç¬¦åˆç°ä»£çš„GPUæ¶æ„ã€‚
å…¶æ¬¡å®ƒå°†ä»»æ„ä¸¤ä¸ªå•è¯é—´çš„ä¾èµ–å˜ä¸ºå¸¸é‡ï¼Œä¸å†æœ‰ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ã€‚

Transformer Blockç»“æ„å¦‚ä¸‹å›¾ï¼š
![Transformer Block](Transformer_Structure.png)
å¯ä»¥çœ‹å‡ºæ€»ä½“ä¸Šåˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼šEncoderï¼ˆå·¦åŠï¼‰å’ŒDecoderï¼ˆå³åŠï¼‰ã€‚
ä¸€ä¸ªå®Œæ•´çš„å¯è®­ç»ƒçš„ç½‘ç»œç”±6ä¸ªEncoderå’ŒDecoderç»„æˆã€‚

ä¸Šé¢é‚£ä¸ªå›¾æ˜¯è®ºæ–‡ä¸­çš„å›¾ï¼Œç¨å¾®æœ‰ç‚¹å¤æ‚ã€‚æŠŠä»–ç®€åŒ–ä¸€ä¸‹ï¼š
![ç®€åŒ–åçš„æ€»ä½“æµç¨‹](The_transformer_encoders_decoders.png)
æ³¨æ„ä¸ºäº†ä¸é‡å¤ç»˜å›¾ï¼Œé‡‡å–äº†å’ŒRNNç»“æ„å›¾ä¸€æ ·çš„ç­–ç•¥ï¼Œå³åªç”»å‡ºäº†ä¸€ä¸ªblockã€‚è€Œå®é™…ä¸Šï¼Œåœ¨å¯¹å¥å­è¿›è¡Œå¤„ç†æ—¶ï¼Œblockæ˜¯åƒç©¿è‘«èŠ¦ä¸²ä¸€æ ·è¿æ¥èµ·æ¥çš„ï¼Œå¹¶ä¸”å¦‚æœåœ¨æœºå™¨ç¿»è¯‘ç­‰ç”Ÿæˆå¼çš„ä»»åŠ¡ä¸­ï¼Œå½“å‰wordçš„è¾“å‡ºéœ€è¦å‰ä¸€ä¸ªwordçš„hidden stateã€‚

å†æ¥è¯´Blocké‡Œé¢çš„å±‚ï¼Œæ¯ä¸ªEncoderæ˜¯ç”±ä¸€å±‚Self-Attentionå’Œä¸€å±‚Feed Forward Neural Network (FFN)ç»„æˆçš„ï¼Œè¿™æ ·å¯ä»¥è·å¾—è¾“å‡ºçš„Attentionè¡¨ç¤ºã€‚
è€Œæ¯ä¸ªDecoderåˆ™æ˜¯ç”±ä¸€å±‚Self Attentionï¼Œä¸€å±‚Encoder-Decoder Attentionå’Œä¸€å±‚FFNç»„æˆçš„ã€‚
å¯ä»¥ç†è§£ä¸ºå…ˆç”¨Self Attentionç†è§£Encoderåçš„å‘é‡ï¼Œç„¶åç”¨Encoder-Decoder Attentionè¿›è¡Œç¿»è¯‘ï¼Œæœ€åç”¨FFNè¾“å‡ºç»“æœã€‚
å…·ä½“çš„æ“ä½œæˆ‘ä»¬åé¢å†ç»†è°ˆã€‚

### Self Attention
Self Attentionæ˜¯Transformeré‡Œé¢æœ€ä¸ºæ ¸å¿ƒçš„å†…å®¹ï¼Œå…¶æ€æƒ³æ˜¯åœ¨ä¸€ä¸ªè¾“å…¥çš„å¥å­å‘é‡ä¸­ï¼Œä¸ºæ¯ä¸ªè¯å‘é‡éƒ½æ‰¾åˆ°ä¸€ä¸ªæƒé‡ï¼Œè¿™ä¸ªæƒé‡ä»£è¡¨å®ƒå’Œå…¶ä»–è¯å‘é‡çš„**å…³è”ç¨‹åº¦**ã€‚
è¿™ä¸ªæƒé‡å‘é‡æˆ‘ä»¬å…ˆç§°å…¶ä¸º$Z$ï¼Œäº§ç”Ÿæ–¹å¼å¦‚ä¸‹ï¼š
$$ Z = Attention(Q, K, V) = Softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V $$
æ³¨æ„åˆ°æˆ‘ä»¬è¿™é‡Œä½¿ç”¨äº†Q, K, Vè¿™ä¸‰ä¸ªå‘é‡æ¥è®¡ç®—ã€‚é‚£ä¹ˆè¿™ä¸‰ä¸ªå‘é‡æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿ
ç®€å•æ¥è¯´å°±æ˜¯é€šè¿‡çº¿æ€§å±‚ç”Ÿæˆçš„ï¼ˆä¸‡èƒ½çš„çŸ©é˜µç›¸ä¹˜ğŸ˜…ï¼‰ï¼Œä»£è¡¨è¯å‘é‡é—´ç›¸å…³æ€§çš„ä¸€äº›å‘é‡ã€‚
è¯å‘é‡çš„`shape = [1, dimension] = [1, 512]`ï¼›æƒå€¼çŸ©é˜µçš„`shape = [512, 64]`ï¼›Q, K, Vçš„`shape = [1, 64]`.
ä¸¾ä¸€ä¸ªåœ¨ç½‘ä¸Šè´­ç‰©å•†åº—ä¹°ä¸œè¥¿çš„ä¾‹å­ï¼šæˆ‘ä»¬è¾“å…¥ä¸€ä¸ªæŸ¥æ‰¾é¡¹Queryï¼Œç³»ç»Ÿæ ¹æ®è¿™ä¸ªQueryåˆ—ä¸¾å‡ºæ•°æ®åº“ä¸­å•†å“çš„Keyï¼Œç„¶åä»ä¸­é€‰å‡ºæˆ‘ä»¬è¦çš„æœ€ç›¸è¿‘çš„é‚£ä¸ªValueã€‚
ç„¶åå†æ¥è§£é‡Šä¸‹ä¸Šé¢äº§ç”Ÿ$Z$çš„é‚£ä¸ªå…¬å¼ï¼Œæˆ‘ä»¬é€šè¿‡$Q$å’Œ$K^{T}$çš„ç‚¹ç§¯è®¡ç®—å‡ºæŸ¥è¯¢å’Œæ•°æ®åº“ä¸­Keyçš„ç›¸ä¼¼æ€§ï¼Œç”¨ dimensionå¼€æ ¹å· $\sqrt{d_{k}}$åšå½’ä¸€åŒ–ï¼Œå¹¶ä¸”é€šè¿‡Softmaxç»™å‡ºä¸€ä¸ªåˆ†æ•°ã€‚
æœ€ç»ˆå°†åˆ†æ•°å’ŒValueç›¸ä¹˜å¾—åˆ°Attentionå‘é‡$Z$ã€‚(æˆ‘ä¸ªäººè®¤ä¸ºValueå‘é‡ååº”äº†è¾“å…¥è¯å‘é‡çš„å…³é”®ç‰¹å¾ï¼Œç±»ä¼¼LSTMä¸­çš„$h_{t}$)

> æ’å…¥ä¸€ç‚¹å†…å®¹æ— å…³çš„è§è§£ï¼Œç°åœ¨å¾ˆå¤šç½‘ç»œçš„è®¾è®¡æ€è·¯è®©æˆ‘æ„Ÿè§‰æ€ªå¼‚çš„ä¸€ç‚¹æ˜¯ï¼Œä¸åŒäºä¼ ç»Ÿç¼–ç¨‹æ˜¯äººå»è®¾è®¡ä¸€ä¸ªç®—æ³•ã€ä¸€ä¸ªæ€è·¯ï¼Œè®¡ç®—æœºå»å®Œç¾åœ°æ‰§è¡Œå®ƒï¼›ç¥ç»ç½‘ç»œä¸­çš„è®¾è®¡æ›´åƒæ˜¯æˆ‘**å‡å®šä½  (NN) èƒ½äº§ç”Ÿä¸€ä¸ªæˆ‘éœ€è¦çš„ç»“æœ**ï¼Œç„¶åæˆ‘åœ¨ä½¿ç”¨è¿™ä¸ªç»“æœçš„åŸºç¡€ä¸Šå»è®¾è®¡ä¸€å¥—ç®—æ³•ï¼Œå¹¶ä¸”å–‚æ•°æ®ã€åå‘ä¼ æ’­è®­ç»ƒï¼Œæœ€åå‘ç°æ¬¸å˜¿ç«Ÿç„¶çœŸçš„workäº†ï¼è¿™ç§å…ˆæœ‰è›‹åæœ‰é¸¡çš„è®¾è®¡æ€è·¯ä¸€ç›´è®©æˆ‘å¾ˆä¸é€‚åº”...
æŒ‰ç…§è¿™ç§æ€è·¯æ¥æ€è€ƒçš„è¯ï¼Œæˆ‘ä»¬è®¾è®¡çš„ç®—æ³•çœŸçš„è´´åˆNNå—ï¼Ÿå¦‚æœNNè‡ªå·±å°±èƒ½åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬çš„ç®—æ³•æ˜¯å¦ç”»è›‡æ·»è¶³äº†å‘¢ï¼Ÿ
**Don't try to understand it. Feel it.** -- ã€ŠTenetã€‹

### Feed Forward Neural Network (FFN)
FFNå°±æ˜¯ä¸€ä¸ªç®€å•çš„ReLU+çº¿æ€§å±‚
$$ FFN(Z) = Linear_{2}(ReLU(Linear_{1}(Z))) = max(0, ZW_{1} + b_{1})W_{2} + b_{2} $$
```python
ffn_1 = nn.Dense(hidden_size, activation='relu')
ffn_2 = nn.Dense(units, activation='identity')
ffn_2(ffn_1(X))
```

[FFNçš„ä½œç”¨](https://zhuanlan.zhihu.com/p/75787683)ï¼š
1. å°†$Z$æ˜ å°„åˆ°ä¸€ä¸ªæ›´å¤§ç»´åº¦çš„ç‰¹å¾ç©ºé—´ï¼Œç„¶åä½¿ç”¨ReLUå¼•å…¥éçº¿æ€§è¿›è¡Œç­›é€‰ï¼Œæœ€åæ¢å¤å›åŸå§‹ç»´åº¦ã€‚
2. Transformeråœ¨æŠ›å¼ƒäº† LSTM ç»“æ„åï¼ŒFFN ä¸­çš„ ReLUæˆä¸ºäº†ä¸€ä¸ªä¸»è¦çš„æä¾›éçº¿æ€§å˜æ¢çš„å•å…ƒã€‚
### Position Encoding & Position Embedding
[è¿™ç¯‡æ–‡ç« ](https://zhuanlan.zhihu.com/p/347904940)å…³äºposition encodingæ€»ç»“çš„ä¸é”™ï¼Œå€¼å¾—ä¸€çœ‹ã€‚
å°½ç®¡è¯´äº†Transformerçš„è¿™ä¹ˆå¤šè®¾è®¡ï¼Œä½†æ˜¯æœ¬è´¨ä¸Šå®ƒä¾ç„¶åªæ˜¯ä¸ªèƒ½åŠ›æ›´å¼ºçš„è¯è¢‹ (CBOW) æ¨¡å‹è€Œå·²ï¼Œå› ä¸ºå…¶ä¸å…·å¤‡æ•æ‰åºåˆ—é¡ºåºçš„èƒ½åŠ›ã€‚
ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒTransformeråœ¨ç¼–ç è¯å‘é‡çš„æ—¶å€™åŠ å…¥äº†ä½ç½®ä¿¡æ¯ï¼Œè¿™ä¸€æŠ€æœ¯ç§°ä¹‹ä¸º`Position Encoding`(è¦å’ŒBertçš„Position EmbeddingåŒºåˆ†å¼€æ¥)ã€‚
è¿™ä¸¤ä¸ªçš„åŒºåˆ«æ˜¯ä¸€ä¸ªæ˜¯å¯ä»¥è§£å†³ä»»æ„é•¿åº¦çš„ä½ç½®ç¼–ç  (encode)ï¼Œè€Œä¸€ä¸ªæ˜¯æ˜ å°„åˆ°å›ºå®šçš„åŒºé—´ã€‚
`Position Encoding`çš„æ–¹å¼æœ‰å¾ˆå¤šï¼Œæœ€ç®€å•æ˜¯ç»å¯¹åæ ‡ç¼–ç `0, 1, 2...`ã€‚
ä½†æ˜¯è¿™æ ·çš„æ–¹å¼ä¼šé€ æˆåœ¨ä½ç½®å‘é‡å åŠ çš„æ—¶å€™å¯¹indexå¤§çš„å…ƒç´ æœ‰åè§ã€‚
Transformeré‡‡ç”¨çš„æ˜¯`sin-cos`è§„åˆ™ï¼Œä½¿ç”¨äº†sinå’Œcosçš„çº¿æ€§å˜æ¢æ¥encodeä½ç½®ä¿¡æ¯ã€‚
> çº¿æ€§å˜æ¢ï¼š
$sin(\alpha+\beta) = sin{\alpha}cos{\beta} + cos{\alpha}sin{\beta}$
$cos(\alpha+\beta) = cos{\alpha}cos{\beta} + sin{\alpha}sin{\beta}$


$$PE(pos, 2i) = sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})$$
$$PE(pos, 2i + 1) = cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})$$
å¯ä»¥æ³¨æ„åˆ°ï¼Œåœ¨ä¸Šé¢è¿™2ä¸ªå…¬å¼ä¸­ï¼Œå¥‡æ•°ä¸ªindexå’Œå¶æ•°ä¸ªindexçš„ç¼–ç æ–¹å¼åˆ†åˆ«é‡‡ç”¨coså’Œsinã€‚å¹¶ä¸”ä¸ºäº†æ‰©å……æ˜ å°„çš„åŒºé—´ï¼ŒåŠ å…¥äº†dimension $d_{model}$è¿™ä¸ªå‚æ•°ã€‚
$d_{model}$è¶Šå¤§ï¼Œæ˜ å°„çš„åŒºé—´å°±è¶Šå¤§ã€‚
`pos`éœ€è¦é™¤ä»¥ä¸€ä¸ªé‡$\theta$ï¼Œå¹¶ä¸”è¿™ä¸ª$\theta \in [1, 10000]$
å¯ä»¥æŠŠä»–ç†è§£ä¸ºåŠ å…¥åå°±åªåœ¨ sin / cos çš„æŸä¸€ä¸ªé¢‘ç‡ä¸Šåšæ˜ å°„ã€‚

ç›´è§‚çš„æ•ˆæœå¯ä»¥çœ‹ä¸‹å›¾ï¼š
![æ˜ å°„åŒºé—´é•¿åº¦äºå¥å­é•¿åº¦ã€dimensionä¹‹é—´çš„å…³ç³»](position_encoding.jpg)

<!-- é¥¿çš„è‚šå­ç–¼ï¼Œå†™ä¸åŠ¨äº†ï¼Œæ˜å¤©ç»§ç»­ğŸ™„ -->

### æ€»ç»“
Transformerçš„ä¼˜åŠ¿ï¼š
  - å¯¹æ¯”LSTM (RNN) 
    å¯å¹¶è¡Œï¼Œå¹¶ä¸”åœ¨è®¡ç®—è¿‡ç¨‹ä¸­ä¸ä¼šå‡ºç°ä¸ä¼šå‡ºç°ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜
  - å¯¹æ¯”CNN
    å¯ä»¥æ•æ‰åˆ°å¥å­ä¾èµ–ï¼Œå…·å¤‡æ›´å¼ºçš„å¯è§£é‡Šæ€§
Transformerçš„ç¼ºç‚¹ï¼š
  - å±€éƒ¨ä¿¡æ¯çš„è·å–ä¸å¦‚RNNå’ŒCNN
  - ä½ç½®ä¿¡æ¯encodeå­˜åœ¨é—®é¢˜
  - é¡¶å±‚æ¢¯åº¦æ¶ˆå¤±
  - å…·æœ‰æ®‹å·®è¿æ¥ï¼Œå¤šæ¬¡è¿ä¹˜æ—¶ä¼šæœ‰æ¢¯åº¦æ¶ˆå¤±é—®é¢˜  

é™„ä¸¤å¼ ç‚«é…·çš„æµç¨‹åŠ¨å›¾
![Transformeræµç¨‹1](transformer_decoding_1.gif)
![Transformeræµç¨‹2](transformer_decoding_2.gif)

## åŸºäºTransformerçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹
ä»‹äºæœ¬æ–‡ç¯‡å¹…ï¼ˆæ‡’ï¼‰ï¼Œè¿™é‡Œå°±ç®€å•ä»‹ç»ä¸‹BERTå®¶æ—åŠç›¸å…³çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„åˆ†ç±»å§ã€‚

- å•å‘ç‰¹å¾è¡¨ç¤ºçš„è‡ªå›å½’é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œç»Ÿç§°ä¸ºå•å‘æ¨¡å‹ï¼š
  ELMO/ULMFiT/SiATL/GPT1.0/GPT2.0
- åŒå‘ç‰¹å¾è¡¨ç¤ºçš„è‡ªç¼–ç é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œç»Ÿç§°ä¸ºBERTç³»åˆ—æ¨¡å‹
  (BERT/MASS/UNILM/ERNIE1.0/ERNIE(THU)/MTDNN/ERNIE2.0/SpanBERT/RoBERTa)
- åŒå‘ç‰¹å¾è¡¨ç¤ºçš„è‡ªå›å½’é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹
  XLNet

![ç°æœ‰çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹](Pretrained_models.jpg)


## Ref 
[1] Attentionæœºåˆ¶ï¼Œhttps://easyai.tech/ai-definition/attention/
[2] A. Vaswani et al., â€œAttention Is All You Need,â€ arXiv:1706.03762 [cs], Dec. 2017, Accessed: Nov. 07, 2020. [Online]. Available: http://arxiv.org/abs/1706.03762.
[3] Youngmi huang, Attention Is All You Needï¼šåŸºæ–¼æ³¨æ„åŠ›æ©Ÿåˆ¶çš„æ©Ÿå™¨ç¿»è­¯æ¨¡å‹, https://cyeninesky3.medium.com/attention-is-all-you-need-%E5%9F%BA%E6%96%BC%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6%E7%9A%84%E6%A9%9F%E5%99%A8%E7%BF%BB%E8%AD%AF%E6%A8%A1%E5%9E%8B-dcc12d251449
[4] å¤§å¸ˆå…„, è¯¦è§£Transformer ï¼ˆAttention Is All You Needï¼‰, https://zhuanlan.zhihu.com/p/48508221
[5] ä¸­äºŒé’å¹´, transformerçš„Position encodingçš„æ€»ç»“, https://zhuanlan.zhihu.com/p/95079337
[6] Jack-Cui, ä¿å§†çº§æ•™ç¨‹ï¼šå›¾è§£Transformer, https://zhuanlan.zhihu.com/p/347904940
[7] Jay Alammar, The Illustrated Transformer, http://jalammar.github.io/illustrated-transformer/
[8] äººå·¥æ™ºèƒ½, Transformer ä¼˜ç¼ºç‚¹åˆ†æ, https://zhuanlan.zhihu.com/p/330483336
[9] JayJay, nlpä¸­çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ€»ç»“(å•å‘æ¨¡å‹ã€BERTç³»åˆ—æ¨¡å‹ã€XLNet), https://zhuanlan.zhihu.com/p/76912493